<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="The AI Talks">
<meta name="description" content="Speaker Xiaojian Ma is a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). He received his Ph.D. in Computer Science at UCLA and a bachelor&rsquo;s degree in Computer Science at Tsinghua University. His research interest primarily focuses on large-scale multimodal learning for understanding, reasoning, and skill learning. In particular, He is interested in building models/agents that can learn from 2D/3D vision and text data, and perform a wide range of reasoning, embodied planning, and control tasks.">
<meta name="keywords" content="The AI Talks, AI, Machine Learning, Computer Science">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Generalist Embodied AI in an Open World"/>
<meta name="twitter:description" content="Speaker Xiaojian Ma is a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). He received his Ph.D. in Computer Science at UCLA and a bachelor&rsquo;s degree in Computer Science at Tsinghua University. His research interest primarily focuses on large-scale multimodal learning for understanding, reasoning, and skill learning. In particular, He is interested in building models/agents that can learn from 2D/3D vision and text data, and perform a wide range of reasoning, embodied planning, and control tasks."/>

<meta property="og:title" content="Generalist Embodied AI in an Open World" />
<meta property="og:description" content="Speaker Xiaojian Ma is a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). He received his Ph.D. in Computer Science at UCLA and a bachelor&rsquo;s degree in Computer Science at Tsinghua University. His research interest primarily focuses on large-scale multimodal learning for understanding, reasoning, and skill learning. In particular, He is interested in building models/agents that can learn from 2D/3D vision and text data, and perform a wide range of reasoning, embodied planning, and control tasks." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://theaitalks.org/talks/2023/1123/" /><meta property="article:section" content="talks" />
<meta property="article:published_time" content="2023-11-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-11-23T00:00:00+00:00" />




  <title>The AI Talks</title>

  
  <link rel="canonical" href="http://theaitalks.org/talks/2023/1123/">
  

  <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.5d9e92b56f30af2bb26c7c6bc87dbb3136248859d5bdf7b663786291f71d6055.css" integrity="sha256-XZ6StW8wryuybHxryH27MTYkiFnVvfe2Y3hikfcdYFU=" crossorigin="anonymous" media="screen" />





  
  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.39e41a7f16bdf8cb16e43cae7d714fa1016f1d2d2898a5b3f27f42c9979204e2.css" integrity="sha256-OeQafxa9&#43;MsW5DyufXFPoQFvHS0omKWz8n9CyZeSBOI=" crossorigin="anonymous" media="screen" />
  



   
  
    
    <link rel="stylesheet" href="/css/custom.min.ed2d2a97f7f0cc62d94e43aee036e965a8d3521cdc17bbb4b7d39e8dab764d99.css" integrity="sha256-7S0ql/fwzGLZTkOu4DbpZajTUhzcF7u0t9Oejat2TZk=" crossorigin="anonymous" media="screen" />
  





  <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">


  

  <meta name="generator" content="Hugo 0.107.0">


  

</head>







<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      The AI Talks
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/organizers/">Organizers</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/speakers/">Speakers</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/talks/">Talks</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/subscribe/">Subscribe</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://theaitalks.org/talks/2023/1123/">
              Generalist Embodied AI in an Open World
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-11-23T00:00:00Z">
                23-11-2023
              </time>
            </span>
          </div>
          <div class="authors">
  <i class="fa fa-user" aria-hidden="true"></i>
    <a href="/authors/xiaojian-ma/">Xiaojian Ma</a></div>

          
          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/embodied-ai/">embodied ai</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h1 id="speaker">
  Speaker
  <a class="heading-link" href="#speaker">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>Xiaojian Ma is a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). He received his Ph.D. in Computer Science at UCLA and a bachelor&rsquo;s degree in Computer Science at Tsinghua University. His research interest primarily focuses on large-scale multimodal learning for understanding, reasoning, and skill learning. In particular, He is interested in building models/agents that can learn from 2D/3D vision and text data, and perform a wide range of reasoning, embodied planning, and control tasks. He has worked at DeepMind, NVIDIA Research, and Google Brain Robotics with a focus on large-scale machine learning. His research has been recognized with the best paper award at the ICML workshop and research fellowships.</p>
<h1 id="abstract">
  Abstract
  <a class="heading-link" href="#abstract">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>From generalist manipulators to humanoids, robotics, and embodied AI is at the center of the stage again but surrounded by a completely different AI landscape, where largely pretrained models like LLMs and VLMs are roaring at multiple fronts of human intelligence. Indeed, embodied AI itself is also experiencing a paradigm shift: from close-world and static settings to more realistic, open-world, and dynamic environments. In this talk, I will present some of our recent efforts to bring more open-endedness to the world of embodied agents. We will first cover SQA3D, a new benchmark for embodied reasoning in 3D scenes. It combines the best of both worlds with open-vocabulary, knowledge-extensive, and situated reasoning and imposes substantial challenges to existing ML models including LLMs. Moving from this foundational groundwork, I will provide some updates on developing open-world generalist embodied agents by leveraging these large models and their principles. Specifically, we explore some key ingredients in developing a vision-based multi-task agent controller in Minecraft, including multimodal fusion and horizon prediction. To further enable solving complex long-term tasks, a hierarchical goal execution agent architecture based on large models is proposed and it becomes one of the best agents so far on the “ObtainDiamond &rsquo;&rsquo; challenge. Finally, I will review some ongoing and possible future directions.</p>
<h1 id="video">
  Video
  <a class="heading-link" href="#video">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FvyBVfgHVLQ?si=W3YFIYRGQ4svMi9A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2022 -
    
    2026
     The AI Talks 
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  
  
  <script src="/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js" integrity="sha256-I2BJOV3DaC&#43;ycZZAhylY4S8fJAZ7sJwyeyM&#43;YpDH7aw="></script>
  

  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7L2H1Q8FMQ"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-7L2H1Q8FMQ', { 'anonymize_ip': false });
}
</script>


  

  

  

  

  

  

  
</body>

</html>
