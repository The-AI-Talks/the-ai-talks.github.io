<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="The AI Talks">
<meta name="description" content="Speaker Ilya is a third year PhD Student in the Princeton University Computational Imaging Lab, advised by Professor Felix Heide, and is an NSF graduate research fellow. His work is at the intersection of computational photography, depth estimation, and designed optics research, modelling the image acquisition pipeline from signal collection to scene reconstruction.
Abstract Modern smartphones can continuously stream 12-megapixel 14-bit RAW images, LiDAR-driven low-resolution depth maps, accelerometer, and gyroscope measurements, estimated lens focal lengths, and a wide array of other camera metadata.">
<meta name="keywords" content="The AI Talks, AI, Machine Learning, Computer Science">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Depth Estimation from Unstabilized Mobile Photography"/>
<meta name="twitter:description" content="Speaker Ilya is a third year PhD Student in the Princeton University Computational Imaging Lab, advised by Professor Felix Heide, and is an NSF graduate research fellow. His work is at the intersection of computational photography, depth estimation, and designed optics research, modelling the image acquisition pipeline from signal collection to scene reconstruction.
Abstract Modern smartphones can continuously stream 12-megapixel 14-bit RAW images, LiDAR-driven low-resolution depth maps, accelerometer, and gyroscope measurements, estimated lens focal lengths, and a wide array of other camera metadata."/>

<meta property="og:title" content="Depth Estimation from Unstabilized Mobile Photography" />
<meta property="og:description" content="Speaker Ilya is a third year PhD Student in the Princeton University Computational Imaging Lab, advised by Professor Felix Heide, and is an NSF graduate research fellow. His work is at the intersection of computational photography, depth estimation, and designed optics research, modelling the image acquisition pipeline from signal collection to scene reconstruction.
Abstract Modern smartphones can continuously stream 12-megapixel 14-bit RAW images, LiDAR-driven low-resolution depth maps, accelerometer, and gyroscope measurements, estimated lens focal lengths, and a wide array of other camera metadata." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://theaitalks.org/talks/2023/0314/" /><meta property="article:section" content="talks" />
<meta property="article:published_time" content="2023-03-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-14T00:00:00+00:00" />




  <title>The AI Talks</title>

  
  <link rel="canonical" href="http://theaitalks.org/talks/2023/0314/">
  

  <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.5d9e92b56f30af2bb26c7c6bc87dbb3136248859d5bdf7b663786291f71d6055.css" integrity="sha256-XZ6StW8wryuybHxryH27MTYkiFnVvfe2Y3hikfcdYFU=" crossorigin="anonymous" media="screen" />





  
  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.39e41a7f16bdf8cb16e43cae7d714fa1016f1d2d2898a5b3f27f42c9979204e2.css" integrity="sha256-OeQafxa9&#43;MsW5DyufXFPoQFvHS0omKWz8n9CyZeSBOI=" crossorigin="anonymous" media="screen" />
  



   
  
    
    <link rel="stylesheet" href="/css/custom.min.ed2d2a97f7f0cc62d94e43aee036e965a8d3521cdc17bbb4b7d39e8dab764d99.css" integrity="sha256-7S0ql/fwzGLZTkOu4DbpZajTUhzcF7u0t9Oejat2TZk=" crossorigin="anonymous" media="screen" />
  





  <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">


  

  <meta name="generator" content="Hugo 0.107.0">


  

</head>







<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      The AI Talks
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/organizers/">Organizers</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/speakers/">Speakers</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/talks/">Talks</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/subscribe/">Subscribe</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://theaitalks.org/talks/2023/0314/">
              Depth Estimation from Unstabilized Mobile Photography
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-03-14T00:00:00Z">
                14-03-2023
              </time>
            </span>
          </div>
          <div class="authors">
  <i class="fa fa-user" aria-hidden="true"></i>
    <a href="/authors/ilya-chugunov/">Ilya Chugunov</a></div>

          
          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/depth-estimation/">depth estimation</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/computational-imaging/">computational imaging</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h1 id="speaker">
  Speaker
  <a class="heading-link" href="#speaker">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>Ilya is a third year PhD Student in the Princeton University Computational Imaging Lab, advised by Professor Felix Heide, and is an NSF graduate research fellow. His work is at the intersection of computational photography, depth estimation, and designed optics research, modelling the image acquisition pipeline from signal collection to scene reconstruction.</p>
<h1 id="abstract">
  Abstract
  <a class="heading-link" href="#abstract">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>Modern smartphones can continuously stream 12-megapixel 14-bit RAW images, LiDAR-driven low-resolution depth maps, accelerometer, and gyroscope measurements, estimated lens focal lengths, and a wide array of other camera metadata. Yet, most research treats smartphone images as just two-dimensional 8-bit RGB arrays and burst photography pipelines only looks at a couple of extra frames, still treating pixel motion as a 2D alignment problem. In our work we look at what we can extract from a “long-burst”, forty-two frames captured in a two-second sequence. We find there is enough parallax information from natural hand tremor alone to recover high-quality scene depth. To this end we propose fitting a simple neural RGB-D scene model directly to this long-burst data to jointly estimate depth and camera motion, with no disjoint pre-processing steps, no feature extraction, and no cost volume estimation.</p>
<h1 id="video">
  Video
  <a class="heading-link" href="#video">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/hwmflLLVQgY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2022 -
    
    2025
     The AI Talks 
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  
  
  <script src="/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js" integrity="sha256-I2BJOV3DaC&#43;ycZZAhylY4S8fJAZ7sJwyeyM&#43;YpDH7aw="></script>
  

  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7L2H1Q8FMQ"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-7L2H1Q8FMQ', { 'anonymize_ip': false });
}
</script>


  

  

  

  

  

  

  
</body>

</html>
