<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="The AI Talks">
<meta name="description" content="Speaker Shusheng Yang is currently a second-year Ph.D. student at NYU Courant, advised by Prof. Saining Xie. His research lies at the intersection of computer vision and multimodal learning, with a specific focus on visual representation learning, spatial intelligence, life-long video understanding, and unified model/world modeling. Please checkout Shusheng’s latest work here: https://scholar.google.com/citations?user=v6dmW5cntoMC&amp;hl=en.
Abstract While MLLMs have advanced the field of video understanding, we demonstrate that they still treat video as sparse frames, underrepresent spatial structure, and rely heavily on textual recall.">
<meta name="keywords" content="The AI Talks, AI, Machine Learning, Computer Science">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Towards Spatial Supersensing in Video"/>
<meta name="twitter:description" content="Speaker Shusheng Yang is currently a second-year Ph.D. student at NYU Courant, advised by Prof. Saining Xie. His research lies at the intersection of computer vision and multimodal learning, with a specific focus on visual representation learning, spatial intelligence, life-long video understanding, and unified model/world modeling. Please checkout Shusheng’s latest work here: https://scholar.google.com/citations?user=v6dmW5cntoMC&amp;hl=en.
Abstract While MLLMs have advanced the field of video understanding, we demonstrate that they still treat video as sparse frames, underrepresent spatial structure, and rely heavily on textual recall."/>

<meta property="og:title" content="Towards Spatial Supersensing in Video" />
<meta property="og:description" content="Speaker Shusheng Yang is currently a second-year Ph.D. student at NYU Courant, advised by Prof. Saining Xie. His research lies at the intersection of computer vision and multimodal learning, with a specific focus on visual representation learning, spatial intelligence, life-long video understanding, and unified model/world modeling. Please checkout Shusheng’s latest work here: https://scholar.google.com/citations?user=v6dmW5cntoMC&amp;hl=en.
Abstract While MLLMs have advanced the field of video understanding, we demonstrate that they still treat video as sparse frames, underrepresent spatial structure, and rely heavily on textual recall." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://theaitalks.org/talks/2025/1216/" /><meta property="article:section" content="talks" />
<meta property="article:published_time" content="2025-12-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-12-16T00:00:00+00:00" />




  <title>The AI Talks</title>

  
  <link rel="canonical" href="http://theaitalks.org/talks/2025/1216/">
  

  <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.5d9e92b56f30af2bb26c7c6bc87dbb3136248859d5bdf7b663786291f71d6055.css" integrity="sha256-XZ6StW8wryuybHxryH27MTYkiFnVvfe2Y3hikfcdYFU=" crossorigin="anonymous" media="screen" />





  
  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.39e41a7f16bdf8cb16e43cae7d714fa1016f1d2d2898a5b3f27f42c9979204e2.css" integrity="sha256-OeQafxa9&#43;MsW5DyufXFPoQFvHS0omKWz8n9CyZeSBOI=" crossorigin="anonymous" media="screen" />
  



   
  
    
    <link rel="stylesheet" href="/css/custom.min.ed2d2a97f7f0cc62d94e43aee036e965a8d3521cdc17bbb4b7d39e8dab764d99.css" integrity="sha256-7S0ql/fwzGLZTkOu4DbpZajTUhzcF7u0t9Oejat2TZk=" crossorigin="anonymous" media="screen" />
  





  <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">


  

  <meta name="generator" content="Hugo 0.107.0">


  

</head>







<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      The AI Talks
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/organizers/">Organizers</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/speakers/">Speakers</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/talks/">Talks</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/subscribe/">Subscribe</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://theaitalks.org/talks/2025/1216/">
              Towards Spatial Supersensing in Video
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2025-12-16T00:00:00Z">
                16-12-2025
              </time>
            </span>
          </div>
          <div class="authors">
  <i class="fa fa-user" aria-hidden="true"></i>
    <a href="/authors/shusheng-yang/">Shusheng Yang</a></div>

          
          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/spatial-supersensing/">Spatial Supersensing</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/predictive-learning/">Predictive Learning</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h1 id="speaker">
  Speaker
  <a class="heading-link" href="#speaker">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>Shusheng Yang is currently a second-year Ph.D. student at NYU Courant, advised by Prof. Saining Xie. His research lies at the intersection of computer vision and multimodal learning, with a specific focus on visual representation learning, spatial intelligence, life-long video understanding, and unified model/world modeling. Please checkout Shusheng’s latest work here: <a href="https://scholar.google.com/citations?user=v6dmW5cntoMC&amp;hl=en">https://scholar.google.com/citations?user=v6dmW5cntoMC&amp;hl=en</a>.</p>
<h1 id="abstract">
  Abstract
  <a class="heading-link" href="#abstract">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>While MLLMs have advanced the field of video understanding, we demonstrate that they still treat video as sparse frames, underrepresent spatial structure, and rely heavily on textual recall. In this work, we propose a hierarchy for &ldquo;spatial supersensing&rdquo; capabilities in video, arguing that genuine video intelligence necessitates not only text-based knowledge recall and semantic perception but also spatial cognition and predictive world modeling. To measure progress, we introduce VSI-SUPER, a benchmark designed for video sequences of arbitrary length. To test whether current limitations stem from data scarcity, we curate VSI-590K and train Cambrian-S; while this model excels on standard benchmarks, it remains limited on VSI-SUPER. Finally, we prototype predictive sensing using latent frame prediction and surprise estimation to handle unbounded visual streams. This approach improves Cambrian-S’s performance on VSI-SUPER and marks an early step toward spatial supersensing.</p>
<h1 id="video">
  Video
  <a class="heading-link" href="#video">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<p>Coming soon. Stay tuned. :-)</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2022 -
    
    2026
     The AI Talks 
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  
  
  <script src="/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js" integrity="sha256-I2BJOV3DaC&#43;ycZZAhylY4S8fJAZ7sJwyeyM&#43;YpDH7aw="></script>
  

  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7L2H1Q8FMQ"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-7L2H1Q8FMQ', { 'anonymize_ip': false });
}
</script>


  

  

  

  

  

  

  
</body>

</html>
