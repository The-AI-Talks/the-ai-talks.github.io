<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Talks on The AI Talks</title>
    <link>http://theaitalks.org/talks/</link>
    <description>Recent content in Talks on The AI Talks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 08 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://theaitalks.org/talks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scaling Beyond Autoregression</title>
      <link>http://theaitalks.org/talks/2025/1208/</link>
      <pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2025/1208/</guid>
      <description>Speaker Jinjie Ni is an AI researcher and dedicated individual contributor at the National University of Singapore, working with Prof. Michael Shieh. His work centers on next generation modeling paradigms and the design of scalable foundation model systems. He is currently focused on several core areas: (1) LLM pretraining, scaling, and architectural innovation; (2) reinforcement learning methods that enhance reasoning capabilities in large language models; (3) and algorithm–system co design aimed at pushing model efficiency and performance.</description>
    </item>
    
    <item>
      <title>Video models are zero-shot learners and reasoners</title>
      <link>http://theaitalks.org/talks/2025/1028/</link>
      <pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2025/1028/</guid>
      <description>Speaker Thaddäus Wiedemer is a 4th-year PhD student in the International Max Planck Research School for Intelligent Systems in Germany, currently interning at Google Deepmind Toronto. Most of his PhD focused on benchmarking robustness and generalization capabilities of large vision-language, language, and video models. His current research explores multi-modal reasoning with video models and how post-training can enable exploration beyond the pretraining data.
Thaddäus’s homepage: https://thaddaeuswiedemer.github.io/
Abstract The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models.</description>
    </item>
    
    <item>
      <title>The Tolman-Sherrington Metamorphosis of Intelligence</title>
      <link>http://theaitalks.org/talks/2025/1023/</link>
      <pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2025/1023/</guid>
      <description>Speaker Hokin Deng is is an independent researcher and founder of Growing AI Like A Child (GrowAI). He was formerly a founding Member of Technical Staff at Myolab.ai, a VC-backed human embodiment intelligence startup. Before that, he worked at Harvard University on single-cell cognition, and previously was a neural engineer at Johns Hopkins Hospital and an affiliated research scientist at Meta Reality Labs for neural wristband. He studied neuroscience and philosophy at JHU.</description>
    </item>
    
    <item>
      <title>Towards Packing the Intelligence of Large Foundation Models on Low Resource Devices</title>
      <link>http://theaitalks.org/talks/2025/0521/</link>
      <pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2025/0521/</guid>
      <description>Speaker Dr. Souvik Kundu (Member IEEE, ACM) is currently a Staff Research scientist at Intel Labs, USA, leading research efforts in scalable and novel AI primitives for foundation models. Souvik received the prestigious AI Rising Star recognition in 2025 from CPAL and Stanford Data Science. He was among the youngest recipients of the Semiconductor Research Corporation Outstanding Industry Liaison Award in 2023. Souvik has played active role in various key efficiency innovations including the LLM KV cache quantization, N:M Sparsity, and efficient long-context understanding generalization for LLMs/VLMs.</description>
    </item>
    
    <item>
      <title>Long video understanding with minimal supervision</title>
      <link>http://theaitalks.org/talks/2023/1221/</link>
      <pubDate>Thu, 21 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1221/</guid>
      <description>Speaker Tengda Han is a post-doctoral research fellow at the Visual Geometry Group at the University of Oxford. He obtained his PhD from the same group in 2022 supervised by Andrew Zisserman. His current research focuses on self-supervised learning, efficient learning, and video understanding.
Abstract Understanding long videos is one of the pinnacles in computer vision. The long-time axis introduces extra challenges compared with images or short videos, and exhaustive manual annotation on long videos is infeasible.</description>
    </item>
    
    <item>
      <title>3D Human Modelling from Image and Text Guidance</title>
      <link>http://theaitalks.org/talks/2023/1207/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1207/</guid>
      <description>Speaker Yukang Cao is a final-year Ph.D. student at The University of Hong Kong with HKU-PS scholarship. He is under the supervision of Dr. Kenneth Wong and works closely with Dr. Kai Han. Throughout his academic journey, he has gained valuable experiences at UT-Austin, PKU, THU, and Tencent. His research interests lie in 3D modelling of the virtual world, with a particular focus on 3D human reconstruction and the generation of 3D avatars.</description>
    </item>
    
    <item>
      <title>Inductive Biases for Learning Long-Horizon Manipulation Skills</title>
      <link>http://theaitalks.org/talks/2023/1130/</link>
      <pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1130/</guid>
      <description>Speaker Murtaza Dalal is a Ph.D. student at Carnegie Mellon University, advised by Ruslan Salakhutdinov. His research is on machine learning for robotics, specifically enabling robot agents to learn and perform long-horizon manipulation behaviors.
Abstract Enabling robots to execute temporally extended sequences of behaviors is a challenging problem for learned systems, due to the difficulty of learning both high-level task information and low-level control. In this talk, I will discuss three approaches that we have developed to address this problem.</description>
    </item>
    
    <item>
      <title>Generalist Embodied AI in an Open World</title>
      <link>http://theaitalks.org/talks/2023/1123/</link>
      <pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1123/</guid>
      <description>Speaker Xiaojian Ma is a research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). He received his Ph.D. in Computer Science at UCLA and a bachelor&amp;rsquo;s degree in Computer Science at Tsinghua University. His research interest primarily focuses on large-scale multimodal learning for understanding, reasoning, and skill learning. In particular, He is interested in building models/agents that can learn from 2D/3D vision and text data, and perform a wide range of reasoning, embodied planning, and control tasks.</description>
    </item>
    
    <item>
      <title>Learning visual language models for video understanding</title>
      <link>http://theaitalks.org/talks/2023/1116/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1116/</guid>
      <description>Speaker Antoine Yang is a Research Scientist at Google DeepMind on the Computer Vision team in London. In 2023, he completed his Ph.D. in the WILLOW team of Inria Paris and École Normale Supérieure. In 2020, he received a double MSc degree in Applied Mathematics from École Polytechnique and ENS Paris-Saclay. He previously interned at Huawei Noah&amp;rsquo;s Ark Lab and Google Research Perception.
Abstract Language models have become increasingly powerful in recent years, but they cannot perceive the world around us.</description>
    </item>
    
    <item>
      <title>3D Structured Generative Models</title>
      <link>http://theaitalks.org/talks/2023/1026/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1026/</guid>
      <description>Speaker Ayush Tewari is a postdoctoral researcher at MIT with Bill Freeman, Josh Tenenbaum, and Vincent Sitzmann. Previously, he completed my Ph.D. at the Max Planck Institute for Informatics with Christian Theobalt.
Abstract My research aims to develop self-supervised methods to perceive and interact with the visual world. I specifically focus on developing 3D generative models that can be learned from 2D data, such as images and videos. I will talk about how this approach enables self-supervised 3D reconstruction of objects and scenes and enables exciting applications.</description>
    </item>
    
    <item>
      <title>Learning to Edit 3D Objects and Scenes</title>
      <link>http://theaitalks.org/talks/2023/1017/</link>
      <pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1017/</guid>
      <description>Speaker Fangyin Wei is a final-year PhD candidate in Computer Science at Princeton University, working with Szymon Rusinkiewicz and Thomas Funkhouser. Fangyin’s research lies in the intersection of computer vision and graphics, with a goal of learning to build a realistic 3D world. Her past research spans topics from generative models for image synthesis/editing to neural renderers for 3D shape synthesis/editing, where she tackled various challenges from learning disentangled representations to learning without labels.</description>
    </item>
    
    <item>
      <title>Multimodal Representation Learning with Deep Generative Models</title>
      <link>http://theaitalks.org/talks/2023/1005/</link>
      <pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/1005/</guid>
      <description>Speaker Shweta is currently a postdoctoral researcher in the Vision Group at the University of British Columbia. She obtained her Ph.D. under the supervision of Prof. Stefan Roth, Ph.D. at Technische Universität Darmstadt. Her Ph.D. thesis has been nominated for the Bertha Benz Best Thesis Award and the GI Dissertation Award 2023. During her Ph.D., she conducted research on deep generative algorithms for multimodal representation learning and the efficiency of exact inference deep generative models.</description>
    </item>
    
    <item>
      <title>Collecting and Leveraging Data without Crowd Workers</title>
      <link>http://theaitalks.org/talks/2023/0914/</link>
      <pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0914/</guid>
      <description>Speaker Yuval Kirstain: During my PhD in generative AI under Professor Omer Levy, I specialized in natural language processing and text-to-image generation. My research has been devoted to developing solutions for acquiring and utilizing task-specific data without the need for traditional crowd workers, employing self-supervised training techniques and gamification instead. My practical experience includes two internships at Facebook AI Research (FAIR), focusing on text-to-image generation and editing. Prior to FAIR, I worked on end-to-end task-oriented dialogue during an internship at IBM Research and gained comprehensive experience in training and evaluating large language models while working at AI21 Labs.</description>
    </item>
    
    <item>
      <title>Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models</title>
      <link>http://theaitalks.org/talks/2023/0831/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0831/</guid>
      <description>Speaker Sheng Shen is a fourth-year Ph.D. student at UC Berkeley, advised by Prof. Kurt Keutzer and Prof. Trevor Darrell. His research interests focus on compute-optimal (multimodal) language modeling, including efficient training/tuning methods, model compression techniques, and the integration of vision-language models. He received the Lotfi A. Zadeh Prize in 2023. Prior to UCB, he obtained a B.S. degree in Electrical Engineering and Computer Science from Peking University.
Sheng&amp;rsquo;s homepage: https://sincerass.</description>
    </item>
    
    <item>
      <title>Learning from Language Models for Visual Intelligence</title>
      <link>http://theaitalks.org/talks/2023/0608/</link>
      <pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0608/</guid>
      <description>Speaker Boyi Li is a Research Scientist at NVIDIA Research and a Postdoctoral Scholar at Berkeley AI Research. Her research interest is in computer vision and machine learning. Her research primarily focuses on multimodal and data-efficient machine learning for building various intelligent systems.
Homepage: https://sites.google.com/site/boyilics/home
Abstract The computer vision community has embraced specialized models trained on fixed object categories like ImageNet or COCO. However, relying solely on visual knowledge may limit flexibility and generality, requiring additional labeled data and hindering user interaction.</description>
    </item>
    
    <item>
      <title>Unsolved ML Safety Problems</title>
      <link>http://theaitalks.org/talks/2023/0525/</link>
      <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0525/</guid>
      <description>Speaker Dan Hendrycks is the director of the Center for AI Safety. He received his PhD from UC Berkeley, where he was advised by Jacob Steinhardt and Dawn Song. His research is supported by the NSF GRFP and the Open Philanthropy AI Fellowship. Dan contributed the GELU activation function, the default activation in nearly all state-of-the-art ML models including BERT, Vision Transformers, and GPT-3. Dan also contributed the main baseline for OOD detection and benchmarks for robustness (ImageNet-C) and large language models (MMLU, MATH).</description>
    </item>
    
    <item>
      <title>3D Generation from Unstructured Single-view Data</title>
      <link>http://theaitalks.org/talks/2023/0511/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0511/</guid>
      <description>Speaker Yinghao Xu is a final-year Ph.D. student at Multimedia Lab (MMLab), Department of Information Engineering in The Chinese University of Hong Kong. His supervisor is Prof. Dahua Lin and Prof. Bolei Zhou. He is very interested in generative models and neural rendering, particularly in 3D generative models. During his Ph.D., he is fortunate to visit Stanford computational group, working with Prof. Gordon Wetzstein. Many of his papers have been awarded as oral representation and best paper candidate at CVPR, ECCV, NeurIPS and ICLR.</description>
    </item>
    
    <item>
      <title>Vision-and-Language Alignment - Towards Universal Multimodal AI</title>
      <link>http://theaitalks.org/talks/2023/0428/</link>
      <pubDate>Fri, 28 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0428/</guid>
      <description>Speaker Junnan Li is currently a senior research manager at Salesforce Research. Before that, he obtained his PhD at the National University of Singapore. His main research focus is in building generative AI models that can understand and generate data in multiple modalities including vision, language, and code. In particular, he is interested in the efficient pre-training of multimodal models. He believes in the value of open-source research. Show less</description>
    </item>
    
    <item>
      <title>Advancing Semi-Supervised Learning: Methods and Benchmarks</title>
      <link>http://theaitalks.org/talks/2023/0413/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0413/</guid>
      <description>Speaker Yidong Wang is a doctoral student at the National Engineering Research Center for Software Engineering of Peking University, advised by Prof. Wei Ye and Prof. Shikun Zhang. He received his master&amp;rsquo;s degree in the Department of Information and Communications Engineering of Tokyo Institute of Technology and his bachelor&amp;rsquo;s degree in the Department of Computer Science and Technology of Nanjing University. His research interests primarily focus on semi-supervised learning, transfer learning, and imbalanced learning.</description>
    </item>
    
    <item>
      <title>Customizing Large-Scale Generative Models</title>
      <link>http://theaitalks.org/talks/2023/0406/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0406/</guid>
      <description>Speaker Nupur Kumari is a Ph.D. student at Carnegie Mellon University, advised by Jun-Yan Zhu. Her research is in generative models, specifically efficient fine-tuning and transfer learning techniques to improve generative models.
Abstract Advancements in large-scale generative models represent a watershed moment. These models can generate a wide variety of objects, styles, and scenes and their compositions. However, as end users, we often wish to synthesize specific concepts from our own personal lives.</description>
    </item>
    
    <item>
      <title>The Case for Reasoning Beyond Recognition</title>
      <link>http://theaitalks.org/talks/2023/0323/</link>
      <pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0323/</guid>
      <description>Speaker Jack Hessel is a Research Scientist at the Allen Institute for AI. He previously earned a PhD from Cornell. These days, he works on improving human-AI collaboration. This includes aligning model behavior with human intent (RLHF), and expanding language models with new modalities (e.g., vision) for a more complete view of the world.
Abstract Algorithms that can jointly process modalities like images+text are needed for next generation search, accessibility, and robot interaction tools.</description>
    </item>
    
    <item>
      <title>On the Gauge Transformation of Neural Fields</title>
      <link>http://theaitalks.org/talks/2023/0316/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0316/</guid>
      <description>Speaker Fangneng Zhan is a postdoctoral researcher at Max Planck Institute for Informatics with Prof. Christian Theobalt. He received his Ph.D. degree of Computer Science &amp;amp; Engineering from Nanyang Technological University, Singapore in 2021. His research interests are Neural Rendering and Generative Models (e.g., NeRF, 3D GAN, Diffusion Models). Aligning with the dictum of “What I cannot create, I do not understand” by Richard Feynman, his research goals are (1) Developing algorithms to synthesize and reconstruct the visual world and (2) Exploring how visual analysis can benefit from the advance of visual synthesis.</description>
    </item>
    
    <item>
      <title>Depth Estimation from Unstabilized Mobile Photography</title>
      <link>http://theaitalks.org/talks/2023/0314/</link>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0314/</guid>
      <description>Speaker Ilya is a third year PhD Student in the Princeton University Computational Imaging Lab, advised by Professor Felix Heide, and is an NSF graduate research fellow. His work is at the intersection of computational photography, depth estimation, and designed optics research, modelling the image acquisition pipeline from signal collection to scene reconstruction.
Abstract Modern smartphones can continuously stream 12-megapixel 14-bit RAW images, LiDAR-driven low-resolution depth maps, accelerometer, and gyroscope measurements, estimated lens focal lengths, and a wide array of other camera metadata.</description>
    </item>
    
    <item>
      <title>Adaptive and Trustworthy NLP with Retrieval for Information Access for Everyone</title>
      <link>http://theaitalks.org/talks/2023/0302/</link>
      <pubDate>Thu, 02 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0302/</guid>
      <description>Speaker Akari Asai is a Ph.D. student in the Paul G. Allen School of Computer Science &amp;amp; Engineering at the University of Washington, advised by Prof. Hannaneh Hajishirzi. Her research lies in natural language processing and machine learning. Her recent research focuses on question answering / IR, multilingual NLP, retrieval-augmented LMs, and efficiency. She received the IBM Fellowship in 2022 and the Nakajima Foundation Fellowship in 2019 and is selected as a 2022 EECS Rising Star.</description>
    </item>
    
    <item>
      <title>Unknown-Aware Learning for Object Detection and Beyond</title>
      <link>http://theaitalks.org/talks/2023/0223/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0223/</guid>
      <description>Speaker Xuefeng Du is a CS Ph.D. student at UW-Madison. His research focus is trustworthy machine learning, such as adversarial robustness, learning problem with label noise, out-of-distribution detection. He is also interested in neural architecture search, graph mining and high-level recognition models, such as object detection and segmentation.
Abstract Out-of-distribution (OOD) detection is indispensable for deploying machine learning models in the wild. One of the key challenges in OOD detection is that models lack supervision signals from unknown data.</description>
    </item>
    
    <item>
      <title>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</title>
      <link>http://theaitalks.org/talks/2023/0216/</link>
      <pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0216/</guid>
      <description>Speaker Linxi &amp;ldquo;Jim&amp;rdquo; Fan is a research scientist at NVIDIA AI. His mission is to build embodied general intelligence. To tackle this grand challenge, his research efforts span foundation models, policy learning, robotics, multimodal learning, and large-scale systems. His latest work &amp;ldquo;MineDojo&amp;rdquo; won the Outstanding Paper Award at NeurIPS 2022. He obtained his Ph.D. degree in Computer Science from Stanford University, advised by Prof. Fei-Fei Li. Previously, Jim did research internships at OpenAI, Google AI, and MILA-Quebec AI Institute.</description>
    </item>
    
    <item>
      <title>Personalizing Text-to-image Generation</title>
      <link>http://theaitalks.org/talks/2023/0209/</link>
      <pubDate>Thu, 09 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0209/</guid>
      <description>Speaker Rinon Gal is a Ph.D. student at Tel Aviv University where he is supervised by Prof. Daniel Cohen-Or and Dr. Amit Bermano. His research focuses on generative models, few-shot and unsupervised approaches, and on combining vision and language. Recently, Rinon has been interning at NVIDIA Research, where he is working on personalization of vision and language models.
Homepage: https://rinongal.github.io
Abstract Text-to-image models offer unprecedented freedom to guide creation through natural language.</description>
    </item>
    
    <item>
      <title>Improving Robustness to Distribution Shifts: Methods and Benchmarks</title>
      <link>http://theaitalks.org/talks/2022/1207/</link>
      <pubDate>Wed, 07 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1207/</guid>
      <description>Speaker Shiori Sagawa is a fourth-year PhD student at Stanford University, advised by Percy Liang. She studies robustness to distribution shifts, and to this end, she has developed methods based on distributionally robust optimization, analyzed these algorithms in the context of deep learning models, and recently built a benchmark on distribution shifts in the wild. She is an Apple PhD Scholar in AI/ML.
Homepage: https://cs.stanford.edu/~ssagawa/.
Abstract Machine learning models deployed in the real world constantly face distribution shifts, yet current models are not robust to these shifts; they can perform well when the train and test distributions are identical, but still have their performance plummet when evaluated on a different test distribution.</description>
    </item>
    
    <item>
      <title>StyleGAN-Based Portrait Image and Video Style Transfer</title>
      <link>http://theaitalks.org/talks/2022/1201/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1201/</guid>
      <description>Speaker Shuai Yang received the B.S. and Ph.D. degrees (Hons.) in computer science from Peking University, Beijing, China, in 2015 and 2020, respectively. He is currently a postdoctoral research fellow with the S-Lab, Nanyang Technological University. Dr. Yang was a Visiting Scholar with the Texas A&amp;amp;M University, from Sep. 2018 to Sep. 2019. He was a Visiting Student with the National Institute of Informatics, Japan, from Mar. 2017 to Aug.</description>
    </item>
    
    <item>
      <title>Principled Solutions for Efficient Artificial Neural Networks</title>
      <link>http://theaitalks.org/talks/2022/1124/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1124/</guid>
      <description>Speaker Bio: Adrian Bulat is currently a Senior Research Scientist at Samsung AI Cambridge. Previously, he received his PhD from the University of Nottingham as part of the Computer Vision Laboratory group. His research work lies at the intersection of Computer Vision and Machine Learning, with work conducted on topics such as efficient neural networks (via bit quantization, network binarization and compression), representation learning and human analysis, where he covered topics such face alignment, face super-resolution and human pose estimation.</description>
    </item>
    
    <item>
      <title>Prompting-based Continual Learning</title>
      <link>http://theaitalks.org/talks/2022/1117/</link>
      <pubDate>Thu, 17 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1117/</guid>
      <description>Speaker Bio: Zifeng Wang is a Ph.D. student at Northeastern University. He received his B.S. degree in Electronic Engineering from Tsinghua University. His research interests include continual (lifelong) learning, data-efficient and parameter-efficient learning, adversarial robustness, and real-world machine learning applications.
Homepage: https://kingspencer.github.io/.
Abstract The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. In this talk, we present a new continual learning paradigm – Prompting-based Continual Learning, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially.</description>
    </item>
    
    <item>
      <title>Finetuning Vision Models: Improving Robustness and Accuracy</title>
      <link>http://theaitalks.org/talks/2022/1013/</link>
      <pubDate>Thu, 13 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1013/</guid>
      <description>Speaker Bio: Mitchell is a fourth year PhD student at the University of Washington. His research interests include large models, transfer learning, and robustness.
Homepage: https://mitchellnw.github.io/.
Abstract I&amp;rsquo;ll discuss methods for fine-tuning which improve model robustness and accuracy. These methods leverage the observation that fine-tuned models often appear to lie in a single low error region. To improve robustness, we therefore interpolate the weights of the pre-trained and fine-tuned models.</description>
    </item>
    
    <item>
      <title>Architectures and Training for Visual Understanding</title>
      <link>http://theaitalks.org/talks/2022/1006/</link>
      <pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1006/</guid>
      <description>Speaker Bio: Hugo Touvron is a research scientist at Meta AI Research. During his PhD he was advised by Hervé Jégou and Matthieu Cord. His current research interests include image classification, transfer learning &amp;amp; fine-grained recognition, with an emphasis on the interplay between architectures and training procedures.
Homepage: https://scholar.google.com/citations?user=xImarzoAAAAJ&amp;amp;hl=en
Abstract Deep learning success is often associated with emblematic architectures. Almost everyone has heard of AlexNet, ResNet or GPT. These successes were also powered by well designed optimisation procedures, which are not usually central to the discussion.</description>
    </item>
    
    <item>
      <title>Bit Diffusion: Generating Discrete Data using Diffusion Models with Analog Bits and Self-Conditioning</title>
      <link>http://theaitalks.org/talks/2022/0929/</link>
      <pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0929/</guid>
      <description>Speaker Bio: Ting Chen is a research scientist in the Google Brain team. His current research interests include self-supervised representation learning, generative modeling, efficient architectures and generalist learning principles. Before joining Google, he received his Ph.D. in Computer Science from UCLA.
Homepage: https://scholar.google.com/citations?user=KoXUMbsAAAAJ&amp;amp;hl=en.
Abstract We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits.</description>
    </item>
    
    <item>
      <title>MMAI: Close the loop for Medical AI application</title>
      <link>http://theaitalks.org/talks/2022/0923/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0923/</guid>
      <description>Speaker Bio: Associate Professor Zongyuan Ge conducts interdisciplinary research at the boundary between medical artificial intelligence, computer-aided diagnosis, biomedical engineering, medical imaging and machine learning and is a multi-award winning medical information science and technology entrepreneur. His research leverages cutting-edge AI technologies using large-scale multi-modality medical data including imaging, medical records, gene data and models the clinicians’ medical knowledge underlying tasks like diagnosis, prognosis and treatment for eye (ophthalmology), skin (dermatology), heart (cardiovascular) and neurodegeneration diseases such as epilepsy and multiple sclerosis.</description>
    </item>
    
    <item>
      <title>Large-Scale Visual Representation Learning with Vision Transformers</title>
      <link>http://theaitalks.org/talks/2022/0922/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0922/</guid>
      <description>Speaker Bio: Xiaohua Zhai is a staff researcher and a manager in the Google Research, Brain team, Zürich. He received his PhD degree from Peking University in 2014. His research interests include large-scale representation learning, multimodal learning, transfer learning and self-supervised learning.
Homepage: https://sites.google.com/view/xzhai.
Abstract Attention-based neural networks such as Vision Transformers (ViT) [1] have recently achieved state-of-the-art results on many computer vision benchmarks (e.g. the Visual Task Adaptation Benchmark [2]).</description>
    </item>
    
    <item>
      <title>Using AI to Diagnose and Assess Parkinson&#39;s Disease: Challenges, Algorithms, and Applications</title>
      <link>http://theaitalks.org/talks/2022/0915/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0915/</guid>
      <description>Speaker Bio: Yuzhe Yang is a PhD student in computer science at MIT CSAIL. He received his B.S. degree in EECS at Peking University. His research interests include machine learning and AI for healthcare. His research focuses on fundamental machine learning algorithms for model robustness and generalization to enable real-world applications especially in health domain, as well as building innovative learning systems to enable new modalities and frameworks for digital health.</description>
    </item>
    
    <item>
      <title>A Vision-and-Language Approach to Computer Vision in the Wild: Modeling &amp; Benchmark</title>
      <link>http://theaitalks.org/talks/2022/0908/</link>
      <pubDate>Thu, 08 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0908/</guid>
      <description>Speaker Bio: Chunyuan Li is currently a Principal Researcher in the Deep Learning Team at Microsoft Research, Redmond. Before that, Chunyuan obtained his PhD at Duke University, working on probabilistic deep learning. He also spent time with Uber AI, Adobe Research, NIST and INRIA. At MSR, Chunyuan is mainly working on large-scale pre-training in computer vision (CV) and vision-language multimodality (MM), with a focus on building transferable vision models that can effortlessly generalize to a wide range of downstream CV &amp;amp; MM tasks.</description>
    </item>
    
  </channel>
</rss>
