<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>video generation on The AI Talks</title>
    <link>http://theaitalks.org/tags/video-generation/</link>
    <description>Recent content in video generation on The AI Talks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 28 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://theaitalks.org/tags/video-generation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Video models are zero-shot learners and reasoners</title>
      <link>http://theaitalks.org/talks/2025/1028/</link>
      <pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2025/1028/</guid>
      <description>Speaker Thaddäus Wiedemer is a 4th-year PhD student in the International Max Planck Research School for Intelligent Systems in Germany, currently interning at Google Deepmind Toronto. Most of his PhD focused on benchmarking robustness and generalization capabilities of large vision-language, language, and video models. His current research explores multi-modal reasoning with video models and how post-training can enable exploration beyond the pretraining data.
Thaddäus’s homepage: https://thaddaeuswiedemer.github.io/
Abstract The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models.</description>
    </item>
    
  </channel>
</rss>
